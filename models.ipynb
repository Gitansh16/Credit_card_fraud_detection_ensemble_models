{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gitansh16/Credit_card_fraud_detection_ensemble_models/blob/main/models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NKzP2JoXtEq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f52MSs5kX4yv",
        "outputId": "7cb1fa35-db0e-490e-ddf2-4fe858ee6d44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
            "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
            "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
            "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
            "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
            "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
            "\n",
            "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
            "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
            "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
            "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
            "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
            "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
            "\n",
            "        V26       V27       V28  Amount  Class  \n",
            "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
            "1  0.125895 -0.008983  0.014724    2.69      0  \n",
            "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
            "3 -0.221929  0.062723  0.061458  123.50      0  \n",
            "4  0.502292  0.219422  0.215153   69.99      0  \n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Correct the file path by enclosing it in quotes\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/creditcard.csv'\n",
        "\n",
        "# Load the CSV file using the correct file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the data\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vjlOkpRhZBr3",
        "outputId": "8770756c-bb63-4399-ed73-d3cfbca95c22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Time      0\n",
              "V1        0\n",
              "V2        0\n",
              "V3        0\n",
              "V4        0\n",
              "V5        0\n",
              "V6        0\n",
              "V7        0\n",
              "V8        0\n",
              "V9        0\n",
              "V10       0\n",
              "V11       0\n",
              "V12       0\n",
              "V13       0\n",
              "V14       0\n",
              "V15       0\n",
              "V16       0\n",
              "V17       0\n",
              "V18       0\n",
              "V19       0\n",
              "V20       0\n",
              "V21       0\n",
              "V22       0\n",
              "V23       0\n",
              "V24       0\n",
              "V25       0\n",
              "V26       0\n",
              "V27       0\n",
              "V28       0\n",
              "Amount    0\n",
              "Class     0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Time</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V5</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V6</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V7</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V8</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V9</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V10</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V11</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V12</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V13</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V14</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V15</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V16</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V17</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V18</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V19</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V20</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V21</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V22</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V23</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V24</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V25</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V26</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V27</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V28</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Amount</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Class</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Comparing Models\n"
      ],
      "metadata": {
        "id": "i1PLh9oynk0L"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBMA9CqTZIxl"
      },
      "source": [
        "##logistic regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgfqHEUhZD3f",
        "outputId": "8dd543de-bf60-4800-e5df-52e5bbed3d71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0: Loss 0.6931471805599453\n",
            "Iteration 100: Loss 0.4964709916457643\n",
            "Iteration 200: Loss 0.374354422235966\n",
            "Iteration 300: Loss 0.2950225828665542\n",
            "Iteration 400: Loss 0.24088503657614677\n",
            "Iteration 500: Loss 0.20225493368043054\n",
            "Iteration 600: Loss 0.17362114173791235\n",
            "Iteration 700: Loss 0.1517100332486345\n",
            "Iteration 800: Loss 0.13449115507481924\n",
            "Iteration 900: Loss 0.1206540329321974\n",
            "Accuracy: 0.9989\n",
            "Confusion Matrix:\n",
            "[[85280    15]\n",
            " [   83    65]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     85295\n",
            "           1       0.81      0.44      0.57       148\n",
            "\n",
            "    accuracy                           1.00     85443\n",
            "   macro avg       0.91      0.72      0.78     85443\n",
            "weighted avg       1.00      1.00      1.00     85443\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset (already balanced using SMOTE)\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = data.drop(columns=['Class']).values\n",
        "y = data['Class'].values\n",
        "\n",
        "# Train-test split (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Apply feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize model parameters\n",
        "def initialize_parameters(n_features):\n",
        "    weights = np.zeros(n_features)\n",
        "    bias = 0\n",
        "    return weights, bias\n",
        "\n",
        "# Sigmoid function with clamping to prevent overflow\n",
        "def sigmoid(z):\n",
        "    return np.clip(1 / (1 + np.exp(-z)), 1e-8, 1 - 1e-8)\n",
        "\n",
        "# Binary cross-entropy loss function\n",
        "def compute_loss(y_true, y_pred):\n",
        "    m = len(y_true)\n",
        "    loss = - (1/m) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "    return loss\n",
        "\n",
        "# Gradient Descent to update weights and bias\n",
        "def gradient_descent(X, y, weights, bias, learning_rate, iterations):\n",
        "    m = X.shape[0]\n",
        "    for i in range(iterations):\n",
        "        # Linear model\n",
        "        z = np.dot(X, weights) + bias\n",
        "        # Sigmoid function to get predictions\n",
        "        y_pred = sigmoid(z)\n",
        "\n",
        "        # Compute gradients\n",
        "        dw = (1/m) * np.dot(X.T, (y_pred - y))\n",
        "        db = (1/m) * np.sum(y_pred - y)\n",
        "\n",
        "        # Update weights and bias\n",
        "        weights -= learning_rate * dw\n",
        "        bias -= learning_rate * db\n",
        "\n",
        "        # Optional: Compute and print loss for every 100 iterations\n",
        "        if i % 100 == 0:\n",
        "            loss = compute_loss(y, y_pred)\n",
        "            print(f'Iteration {i}: Loss {loss}')\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "# Model training\n",
        "def train(X_train, y_train, learning_rate=0.01, iterations=1000):\n",
        "    n_features = X_train.shape[1]\n",
        "    weights, bias = initialize_parameters(n_features)\n",
        "    weights, bias = gradient_descent(X_train, y_train, weights, bias, learning_rate, iterations)\n",
        "    return weights, bias\n",
        "\n",
        "# Predict function\n",
        "def predict(X, weights, bias):\n",
        "    z = np.dot(X, weights) + bias\n",
        "    y_pred = sigmoid(z)\n",
        "    y_pred_class = [1 if i > 0.5 else 0 for i in y_pred]\n",
        "    return np.array(y_pred_class)\n",
        "\n",
        "# Train the model\n",
        "weights, bias = train(X_train, y_train, learning_rate=0.01, iterations=1000)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = predict(X_test, weights, bias)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Output the results\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)\n",
        "print('Classification Report:')\n",
        "print(class_report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Parameter Tunning"
      ],
      "metadata": {
        "id": "wx84r21XUKbn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7u55pPRaEJH",
        "outputId": "76928fad-d2eb-4102-946d-dd2442760c2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing with learning rate: 0.001, iterations: 100\n",
            "Accuracy: 0.999283727985169 with learning rate: 0.001, iterations: 100\n",
            "Testing with learning rate: 0.001, iterations: 200\n",
            "Accuracy: 0.999283727985169 with learning rate: 0.001, iterations: 200\n",
            "Testing with learning rate: 0.001, iterations: 300\n",
            "Accuracy: 0.999283727985169 with learning rate: 0.001, iterations: 300\n",
            "Testing with learning rate: 0.01, iterations: 100\n",
            "Accuracy: 0.999283727985169 with learning rate: 0.01, iterations: 100\n",
            "Testing with learning rate: 0.01, iterations: 200\n",
            "Accuracy: 0.999283727985169 with learning rate: 0.01, iterations: 200\n",
            "Testing with learning rate: 0.01, iterations: 300\n",
            "Accuracy: 0.999283727985169 with learning rate: 0.01, iterations: 300\n",
            "Testing with learning rate: 0.05, iterations: 100\n",
            "Accuracy: 0.999283727985169 with learning rate: 0.05, iterations: 100\n",
            "Testing with learning rate: 0.05, iterations: 200\n",
            "Accuracy: 0.999283727985169 with learning rate: 0.05, iterations: 200\n",
            "Testing with learning rate: 0.05, iterations: 300\n",
            "Accuracy: 0.999283727985169 with learning rate: 0.05, iterations: 300\n",
            "Testing with learning rate: 0.1, iterations: 100\n",
            "Accuracy: 0.999283727985169 with learning rate: 0.1, iterations: 100\n",
            "Testing with learning rate: 0.1, iterations: 200\n",
            "Accuracy: 0.999283727985169 with learning rate: 0.1, iterations: 200\n",
            "Testing with learning rate: 0.1, iterations: 300\n",
            "Accuracy: 0.999283727985169 with learning rate: 0.1, iterations: 300\n",
            "Best Learning Rate: 0.001\n",
            "Best Iterations: 100\n",
            "Best Accuracy: 0.999283727985169\n",
            "Confusion Matrix:\n",
            "[[71071    11]\n",
            " [   40    80]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     71082\n",
            "           1       0.88      0.67      0.76       120\n",
            "\n",
            "    accuracy                           1.00     71202\n",
            "   macro avg       0.94      0.83      0.88     71202\n",
            "weighted avg       1.00      1.00      1.00     71202\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "\n",
        "\n",
        "# Extract independent and dependent variables\n",
        "x = data.iloc[:, :-1].values  # All columns except the last one (which is the target 'Class')\n",
        "y = data.iloc[:, -1].values   # Last column (Class: 0 for non-fraud, 1 for fraud)\n",
        "\n",
        "# Feature scaling (Standardizing the data)\n",
        "scaler = StandardScaler()\n",
        "x = scaler.fit_transform(x)\n",
        "\n",
        "# Splitting dataset into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n",
        "\n",
        "# Adding a bias term (intercept) to the dataset\n",
        "x_train = np.c_[np.ones((x_train.shape[0], 1)), x_train]  # Adding 1 for intercept\n",
        "x_test = np.c_[np.ones((x_test.shape[0], 1)), x_test]\n",
        "\n",
        "# Logistic regression hyperparameter tuning\n",
        "def logistic_regression_tuning(x_train, y_train, x_test, y_test, learning_rates, max_iter_list):\n",
        "    best_accuracy = 0\n",
        "    best_lr = None\n",
        "    best_iterations = None\n",
        "    best_model = None\n",
        "\n",
        "    # Try all combinations of learning rates and iterations\n",
        "    for lr in learning_rates:\n",
        "        for max_iter in max_iter_list:\n",
        "            print(f\"Testing with learning rate: {lr}, iterations: {max_iter}\")\n",
        "            model = LogisticRegression(solver='lbfgs', max_iter=max_iter, C=1/lr)\n",
        "            model.fit(x_train, y_train)\n",
        "            accuracy = model.score(x_test, y_test)\n",
        "\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_lr = lr\n",
        "                best_iterations = max_iter\n",
        "                best_model = model\n",
        "\n",
        "            print(f\"Accuracy: {accuracy} with learning rate: {lr}, iterations: {max_iter}\")\n",
        "\n",
        "    return best_model, best_lr, best_iterations, best_accuracy\n",
        "\n",
        "# Set ranges for hyperparameters\n",
        "learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
        "max_iter_list = [100, 200, 300]\n",
        "\n",
        "# Perform hyperparameter tuning using logistic regression\n",
        "best_model, best_lr, best_iterations, best_accuracy = logistic_regression_tuning(\n",
        "    x_train, y_train, x_test, y_test, learning_rates, max_iter_list)\n",
        "\n",
        "# Evaluate the best model\n",
        "y_pred = best_model.predict(x_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "# Output the best results\n",
        "print(f\"Best Learning Rate: {best_lr}\")\n",
        "print(f\"Best Iterations: {best_iterations}\")\n",
        "print(f\"Best Accuracy: {best_accuracy}\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n",
        "print(f\"Classification Report:\\n{classification_rep}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7ig4E77adQt"
      },
      "source": [
        "##Randomforest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h9KUT5i1acAc",
        "outputId": "936836cb-9846-4a54-f1f3-cbd961f30908"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Time</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V5</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V6</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V7</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V8</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V9</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V10</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V11</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V12</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V13</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V14</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V15</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V16</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V17</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V18</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V19</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V20</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V21</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V22</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V23</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V24</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V25</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V26</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V27</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V28</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Amount</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Class</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "Time      0\n",
              "V1        0\n",
              "V2        0\n",
              "V3        0\n",
              "V4        0\n",
              "V5        0\n",
              "V6        0\n",
              "V7        0\n",
              "V8        0\n",
              "V9        0\n",
              "V10       0\n",
              "V11       0\n",
              "V12       0\n",
              "V13       0\n",
              "V14       0\n",
              "V15       0\n",
              "V16       0\n",
              "V17       0\n",
              "V18       0\n",
              "V19       0\n",
              "V20       0\n",
              "V21       0\n",
              "V22       0\n",
              "V23       0\n",
              "V24       0\n",
              "V25       0\n",
              "V26       0\n",
              "V27       0\n",
              "V28       0\n",
              "Amount    0\n",
              "Class     0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCTiPX9yaIG-",
        "outputId": "db431079-3c3e-4570-a960-31f7858048fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0: Cost 0.6906586376064595\n",
            "Iteration 100: Cost 0.4949876059280775\n",
            "Iteration 200: Cost 0.3734486687223002\n",
            "Iteration 300: Cost 0.29445296291651774\n",
            "Iteration 400: Cost 0.2405196791893254\n",
            "Iteration 500: Cost 0.20202008216979592\n",
            "Iteration 600: Cost 0.1734737593163031\n",
            "Iteration 700: Cost 0.15162384314520327\n",
            "Iteration 800: Cost 0.1344494109697371\n",
            "Iteration 900: Cost 0.12064564070149646\n",
            "Confusion Matrix:\n",
            "[[71073     9]\n",
            " [   61    59]]\n",
            "Accuracy: 0.9990168815482711\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     71082\n",
            "           1       0.87      0.49      0.63       120\n",
            "\n",
            "    accuracy                           1.00     71202\n",
            "   macro avg       0.93      0.75      0.81     71202\n",
            "weighted avg       1.00      1.00      1.00     71202\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Extract independent and dependent variables\n",
        "x = data.iloc[:, :-1].values  # All columns except the last one (which is the target 'Class')\n",
        "y = data.iloc[:, -1].values   # Last column (Class: 0 for non-fraud, 1 for fraud)\n",
        "\n",
        "# Feature scaling (Standardizing the data)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x = scaler.fit_transform(x)\n",
        "\n",
        "# Splitting dataset into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n",
        "\n",
        "# Adding a bias term (intercept) to the dataset\n",
        "x_train = np.c_[np.ones((x_train.shape[0], 1)), x_train]  # Adding 1 for intercept\n",
        "x_test = np.c_[np.ones((x_test.shape[0], 1)), x_test]\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Hypothesis function\n",
        "def hypothesis(x, theta):\n",
        "    return sigmoid(np.dot(x, theta))\n",
        "\n",
        "# Cost function (Log-Loss)\n",
        "def cost_function(x, y, theta):\n",
        "    m = len(y)\n",
        "    h = hypothesis(x, theta)\n",
        "    return - (1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
        "\n",
        "# Gradient Descent\n",
        "def gradient_descent(x, y, theta, learning_rate, num_iterations):\n",
        "    m = len(y)\n",
        "    cost_history = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        gradient = (1 / m) * np.dot(x.T, (hypothesis(x, theta) - y))\n",
        "        theta -= learning_rate * gradient\n",
        "        cost = cost_function(x, y, theta)\n",
        "        cost_history.append(cost)\n",
        "        if i % 100 == 0:  # Print cost every 100 iterations\n",
        "            print(f\"Iteration {i}: Cost {cost}\")\n",
        "\n",
        "    return theta, cost_history\n",
        "\n",
        "# Prediction function\n",
        "def predict(x, theta):\n",
        "    return hypothesis(x, theta) >= 0.5\n",
        "\n",
        "# Initialize theta (weights)\n",
        "theta = np.zeros(x_train.shape[1])\n",
        "\n",
        "# Set hyperparameters\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "\n",
        "# Train the model\n",
        "theta, cost_history = gradient_descent(x_train, y_train, theta, learning_rate, num_iterations)\n",
        "\n",
        "# Predicting the test results\n",
        "y_pred = predict(x_test, theta)\n",
        "\n",
        "# Evaluating the model\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Parameter Tunning"
      ],
      "metadata": {
        "id": "N2PTvzBpUE_D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_8o_2Ima4s4",
        "outputId": "cf8c5a62-f05d-4445-fd7b-dd40265884b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing with learning rate: 0.001, iterations: 500\n",
            "Iteration 0: Cost 0.6928980459464835\n",
            "Iteration 100: Cost 0.6686032714532323\n",
            "Iteration 200: Cost 0.645492719808075\n",
            "Iteration 300: Cost 0.6235067321865587\n",
            "Iteration 400: Cost 0.6025876358425402\n",
            "Accuracy: 0.9989747479003399 with learning rate: 0.001, iterations: 500\n",
            "Testing with learning rate: 0.001, iterations: 1000\n",
            "Iteration 0: Cost 0.6928980459464835\n",
            "Iteration 100: Cost 0.6686032714532323\n",
            "Iteration 200: Cost 0.645492719808075\n",
            "Iteration 300: Cost 0.6235067321865587\n",
            "Iteration 400: Cost 0.6025876358425402\n",
            "Iteration 500: Cost 0.5826798849811325\n",
            "Iteration 600: Cost 0.5637301586289974\n",
            "Iteration 700: Cost 0.5456874210556855\n",
            "Iteration 800: Cost 0.5285029500846758\n",
            "Iteration 900: Cost 0.5121303383050443\n",
            "Accuracy: 0.9989747479003399 with learning rate: 0.001, iterations: 1000\n",
            "Testing with learning rate: 0.001, iterations: 1500\n",
            "Iteration 0: Cost 0.6928980459464835\n",
            "Iteration 100: Cost 0.6686032714532323\n",
            "Iteration 200: Cost 0.645492719808075\n",
            "Iteration 300: Cost 0.6235067321865587\n",
            "Iteration 400: Cost 0.6025876358425402\n",
            "Iteration 500: Cost 0.5826798849811325\n",
            "Iteration 600: Cost 0.5637301586289974\n",
            "Iteration 700: Cost 0.5456874210556855\n",
            "Iteration 800: Cost 0.5285029500846758\n",
            "Iteration 900: Cost 0.5121303383050443\n",
            "Iteration 1000: Cost 0.4965254717938932\n",
            "Iteration 1100: Cost 0.4816464905183018\n",
            "Iteration 1200: Cost 0.4674537341290684\n",
            "Iteration 1300: Cost 0.4539096764062856\n",
            "Iteration 1400: Cost 0.4409788511826561\n",
            "Accuracy: 0.9989747479003399 with learning rate: 0.001, iterations: 1500\n",
            "Testing with learning rate: 0.01, iterations: 500\n",
            "Iteration 0: Cost 0.6906586376064595\n",
            "Iteration 100: Cost 0.4949876059280775\n",
            "Iteration 200: Cost 0.3734486687223002\n",
            "Iteration 300: Cost 0.29445296291651774\n",
            "Iteration 400: Cost 0.2405196791893254\n",
            "Accuracy: 0.9989747479003399 with learning rate: 0.01, iterations: 500\n",
            "Testing with learning rate: 0.01, iterations: 1000\n",
            "Iteration 0: Cost 0.6906586376064595\n",
            "Iteration 100: Cost 0.4949876059280775\n",
            "Iteration 200: Cost 0.3734486687223002\n",
            "Iteration 300: Cost 0.29445296291651774\n",
            "Iteration 400: Cost 0.2405196791893254\n",
            "Iteration 500: Cost 0.20202008216979592\n",
            "Iteration 600: Cost 0.1734737593163031\n",
            "Iteration 700: Cost 0.15162384314520327\n",
            "Iteration 800: Cost 0.1344494109697371\n",
            "Iteration 900: Cost 0.12064564070149646\n",
            "Accuracy: 0.9990168815482711 with learning rate: 0.01, iterations: 1000\n",
            "Testing with learning rate: 0.01, iterations: 1500\n",
            "Iteration 0: Cost 0.6906586376064595\n",
            "Iteration 100: Cost 0.4949876059280775\n",
            "Iteration 200: Cost 0.3734486687223002\n",
            "Iteration 300: Cost 0.29445296291651774\n",
            "Iteration 400: Cost 0.2405196791893254\n",
            "Iteration 500: Cost 0.20202008216979592\n",
            "Iteration 600: Cost 0.1734737593163031\n",
            "Iteration 700: Cost 0.15162384314520327\n",
            "Iteration 800: Cost 0.1344494109697371\n",
            "Iteration 900: Cost 0.12064564070149646\n",
            "Iteration 1000: Cost 0.10933973403305274\n",
            "Iteration 1100: Cost 0.09992941747899624\n",
            "Iteration 1200: Cost 0.09198758509743753\n",
            "Iteration 1300: Cost 0.0852039936365487\n",
            "Iteration 1400: Cost 0.07934846555404472\n",
            "Accuracy: 0.9990168815482711 with learning rate: 0.01, iterations: 1500\n",
            "Testing with learning rate: 0.05, iterations: 500\n",
            "Iteration 0: Cost 0.680766756770622\n",
            "Iteration 100: Cost 0.20005061165339547\n",
            "Iteration 200: Cost 0.10860374004663635\n",
            "Iteration 300: Cost 0.07386840459587253\n",
            "Iteration 400: Cost 0.05602329609203278\n",
            "Accuracy: 0.9990590151962023 with learning rate: 0.05, iterations: 500\n",
            "Testing with learning rate: 0.05, iterations: 1000\n",
            "Iteration 0: Cost 0.680766756770622\n",
            "Iteration 100: Cost 0.20005061165339547\n",
            "Iteration 200: Cost 0.10860374004663635\n",
            "Iteration 300: Cost 0.07386840459587253\n",
            "Iteration 400: Cost 0.05602329609203278\n",
            "Iteration 500: Cost 0.045261513553492456\n",
            "Iteration 600: Cost 0.03809827718786907\n",
            "Iteration 700: Cost 0.033001323953689814\n",
            "Iteration 800: Cost 0.029196027074233355\n",
            "Iteration 900: Cost 0.02625015017155569\n",
            "Accuracy: 0.999115193393444 with learning rate: 0.05, iterations: 1000\n",
            "Testing with learning rate: 0.05, iterations: 1500\n",
            "Iteration 0: Cost 0.680766756770622\n",
            "Iteration 100: Cost 0.20005061165339547\n",
            "Iteration 200: Cost 0.10860374004663635\n",
            "Iteration 300: Cost 0.07386840459587253\n",
            "Iteration 400: Cost 0.05602329609203278\n",
            "Iteration 500: Cost 0.045261513553492456\n",
            "Iteration 600: Cost 0.03809827718786907\n",
            "Iteration 700: Cost 0.033001323953689814\n",
            "Iteration 800: Cost 0.029196027074233355\n",
            "Iteration 900: Cost 0.02625015017155569\n",
            "Iteration 1000: Cost 0.023904084802854487\n",
            "Iteration 1100: Cost 0.021992752442465707\n",
            "Iteration 1200: Cost 0.020406294760485815\n",
            "Iteration 1300: Cost 0.019068832545436055\n",
            "Iteration 1400: Cost 0.017926308934069747\n",
            "Accuracy: 0.9991573270413753 with learning rate: 0.05, iterations: 1500\n",
            "Best Learning Rate: 0.05\n",
            "Best Iterations: 1500\n",
            "Best Accuracy: 0.9991573270413753\n",
            "Confusion Matrix:\n",
            "[[71072    10]\n",
            " [   50    70]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     71082\n",
            "           1       0.88      0.58      0.70       120\n",
            "\n",
            "    accuracy                           1.00     71202\n",
            "   macro avg       0.94      0.79      0.85     71202\n",
            "weighted avg       1.00      1.00      1.00     71202\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "\n",
        "\n",
        "# Extract independent and dependent variables\n",
        "x = data.iloc[:, :-1].values  # All columns except the last one (target 'Class')\n",
        "y = data.iloc[:, -1].values   # Last column (Class: 0 for non-fraud, 1 for fraud)\n",
        "\n",
        "# Feature scaling (Standardizing the data)\n",
        "scaler = StandardScaler()\n",
        "x = scaler.fit_transform(x)\n",
        "\n",
        "# Splitting dataset into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n",
        "\n",
        "# Adding a bias term (intercept) to the dataset\n",
        "x_train = np.c_[np.ones((x_train.shape[0], 1)), x_train]  # Adding 1 for intercept\n",
        "x_test = np.c_[np.ones((x_test.shape[0], 1)), x_test]\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Hypothesis function\n",
        "def hypothesis(x, theta):\n",
        "    return sigmoid(np.dot(x, theta))\n",
        "\n",
        "# Cost function (Log-Loss)\n",
        "def cost_function(x, y, theta):\n",
        "    m = len(y)\n",
        "    h = hypothesis(x, theta)\n",
        "    return - (1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
        "\n",
        "# Gradient Descent\n",
        "def gradient_descent(x, y, theta, learning_rate, num_iterations):\n",
        "    m = len(y)\n",
        "    cost_history = []\n",
        "    for i in range(num_iterations):\n",
        "        gradient = (1 / m) * np.dot(x.T, (hypothesis(x, theta) - y))\n",
        "        theta -= learning_rate * gradient\n",
        "        cost = cost_function(x, y, theta)\n",
        "        cost_history.append(cost)\n",
        "        if i % 100 == 0:  # Print cost every 100 iterations\n",
        "            print(f\"Iteration {i}: Cost {cost}\")\n",
        "    return theta, cost_history\n",
        "\n",
        "# Prediction function\n",
        "def predict(x, theta):\n",
        "    return hypothesis(x, theta) >= 0.5\n",
        "\n",
        "# Hyperparameter tuning function\n",
        "def hyperparameter_tuning(x_train, y_train, x_test, y_test, learning_rates, num_iterations_list):\n",
        "    best_accuracy = 0\n",
        "    best_params = None\n",
        "    best_theta = None\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        for num_iter in num_iterations_list:\n",
        "            print(f\"Testing with learning rate: {lr}, iterations: {num_iter}\")\n",
        "            theta = np.zeros(x_train.shape[1])\n",
        "            theta, _ = gradient_descent(x_train, y_train, theta, lr, num_iter)\n",
        "            y_pred = predict(x_test, theta)\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_params = (lr, num_iter)\n",
        "                best_theta = theta\n",
        "\n",
        "            print(f\"Accuracy: {accuracy} with learning rate: {lr}, iterations: {num_iter}\")\n",
        "\n",
        "    return best_theta, best_params, best_accuracy\n",
        "\n",
        "# Set hyperparameter ranges\n",
        "learning_rates = [0.001, 0.01, 0.05]\n",
        "num_iterations_list = [500, 1000, 1500]\n",
        "\n",
        "# Perform hyperparameter tuning\n",
        "best_theta, best_params, best_accuracy = hyperparameter_tuning(x_train, y_train, x_test, y_test, learning_rates, num_iterations_list)\n",
        "\n",
        "# Predicting the test results with the best parameters\n",
        "y_pred = predict(x_test, best_theta)\n",
        "\n",
        "# Evaluating the model\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "# Output the best results\n",
        "print(f\"Best Learning Rate: {best_params[0]}\")\n",
        "print(f\"Best Iterations: {best_params[1]}\")\n",
        "print(f\"Best Accuracy: {best_accuracy}\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n",
        "print(f\"Classification Report:\\n{classification_rep}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_ocH5GEnGSz"
      },
      "source": [
        "##SVM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1F7L9AKpgwh",
        "outputId": "fe4e6bb2-cc15-46f7-b48b-638777000e48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions:  [0 0 0 ... 0 0 0]\n",
            "Accuracy: 99.94%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     71082\n",
            "           1       0.83      0.81      0.82       120\n",
            "\n",
            "    accuracy                           1.00     71202\n",
            "   macro avg       0.91      0.90      0.91     71202\n",
            "weighted avg       1.00      1.00      1.00     71202\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "\n",
        "# Extracting Independent and dependent Variables\n",
        "x = data.drop(columns=['Class']).values  # All columns except 'Class' are features\n",
        "y = data['Class'].values  # 'Class' column is the target variable\n",
        "\n",
        "# Splitting the dataset into training and test set\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n",
        "\n",
        "# Feature Scaling\n",
        "st_x = StandardScaler()\n",
        "x_train = st_x.fit_transform(x_train)\n",
        "x_test = st_x.transform(x_test)\n",
        "\n",
        "# Create and train the SVM classifier\n",
        "classifier = SVC(kernel='linear', random_state=0)\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "# Output the predictions (if needed)\n",
        "print(\"Predictions: \", y_pred)\n",
        "\n",
        "# Evaluate the performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##XGBoost"
      ],
      "metadata": {
        "id": "3t0UzuUOVPAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/creditcard.csv')\n",
        "\n",
        "# Feature and target variables\n",
        "X = data.drop(columns=['Class'])  # Features\n",
        "y = data['Class']  # Target\n",
        "\n",
        "# Splitting the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize 'Time' and 'Amount' columns (important for XGBoost's performance)\n",
        "scaler = StandardScaler()\n",
        "X_train[['Time', 'Amount']] = scaler.fit_transform(X_train[['Time', 'Amount']])\n",
        "X_test[['Time', 'Amount']] = scaler.transform(X_test[['Time', 'Amount']])\n",
        "\n",
        "# Initialize XGBoost model\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    n_estimators=100,        # Number of boosting rounds\n",
        "    max_depth=6,             # Maximum depth of a tree\n",
        "    learning_rate=0.1,       # Step size shrinkage\n",
        "    subsample=0.8,           # Subsample ratio of the training instances\n",
        "    colsample_bytree=0.8,    # Subsample ratio of columns for each tree\n",
        "    scale_pos_weight=len(y_train) / sum(y_train),  # Handle class imbalance\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Display classification report (precision, recall, F1-score)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QzRQD7rVOky",
        "outputId": "50d1d00b-ac88-4c59-f9d3-602933902a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 99.93%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     71079\n",
            "           1       0.79      0.84      0.81       123\n",
            "\n",
            "    accuracy                           1.00     71202\n",
            "   macro avg       0.90      0.92      0.91     71202\n",
            "weighted avg       1.00      1.00      1.00     71202\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ddPJH_tjZi1"
      },
      "source": [
        "#Combined Models- Proposed Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ24gepiSR-m"
      },
      "source": [
        "##Voting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbKd1tCLYwf6",
        "outputId": "558e8952-99c9-43b0-cd1c-ecadce57dc70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Classifier Accuracy: 99.94%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     71082\n",
            "           1       0.86      0.78      0.82       120\n",
            "\n",
            "    accuracy                           1.00     71202\n",
            "   macro avg       0.93      0.89      0.91     71202\n",
            "weighted avg       1.00      1.00      1.00     71202\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import Counter\n",
        "from scipy.special import expit  # For the sigmoid function\n",
        "\n",
        "# Assuming 'data' is your dataset\n",
        "x = data.drop(columns=['Class']).values  # All columns except 'Class'\n",
        "y = data['Class'].values  # 'Class' column is the target variable\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "x = scaler.fit_transform(x)\n",
        "\n",
        "# Split the dataset\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n",
        "\n",
        "# Logistic Regression (from scratch)\n",
        "class LogisticRegressionScratch:\n",
        "    def __init__(self, learning_rate=0.01, iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.iterations = iterations\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return expit(z)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.theta = np.zeros(X.shape[1])\n",
        "        for i in range(self.iterations):\n",
        "            z = np.dot(X, self.theta)\n",
        "            predictions = self.sigmoid(z)\n",
        "            gradient = np.dot(X.T, (predictions - y)) / len(y)\n",
        "            self.theta -= self.learning_rate * gradient\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return self.sigmoid(np.dot(X, self.theta))\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.predict_proba(X) >= 0.5\n",
        "\n",
        "# Random Forest (simplified from scratch)\n",
        "class DecisionTreeScratch:\n",
        "    def fit(self, X, y):\n",
        "        self.feature_idx = np.random.choice(X.shape[1], X.shape[1] // 2, replace=False)\n",
        "        self.threshold = np.median(X[:, self.feature_idx])\n",
        "        self.label = Counter(y).most_common(1)[0][0]\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.ones(X.shape[0]) * self.label\n",
        "        return predictions\n",
        "\n",
        "class RandomForestScratch:\n",
        "    def __init__(self, n_estimators=10):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for _ in range(self.n_estimators):\n",
        "            idxs = np.random.choice(X.shape[0], X.shape[0] // 2, replace=True)\n",
        "            tree = DecisionTreeScratch()\n",
        "            tree.fit(X[idxs], y[idxs])\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        majority_vote = np.round(np.mean(tree_preds, axis=0))\n",
        "        return majority_vote\n",
        "\n",
        "# Support Vector Machine (SVM from scratch)\n",
        "\n",
        "class SVMScratch:\n",
        "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.lambda_param = lambda_param\n",
        "        self.n_iters = n_iters\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y_ = np.where(y <= 0, -1, 1)\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "        self.b = 0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
        "                if condition:\n",
        "                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w)\n",
        "                else:\n",
        "                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
        "                    self.b -= self.learning_rate * y_[idx]\n",
        "\n",
        "    def predict(self, X):\n",
        "        approx = np.dot(X, self.w) - self.b\n",
        "        return np.sign(approx)\n",
        "\n",
        "# Voting Classifier (Soft Voting)\n",
        "class VotingClassifierScratch:\n",
        "    def __init__(self, classifiers):\n",
        "        self.classifiers = classifiers\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for clf in self.classifiers:\n",
        "            clf.fit(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Collect probabilities from all classifiers\n",
        "        preds = np.zeros((X.shape[0], len(self.classifiers)))\n",
        "\n",
        "        for i, clf in enumerate(self.classifiers):\n",
        "            # Use predict_proba for Logistic Regression, else predict for others\n",
        "            if hasattr(clf, 'predict_proba'):\n",
        "                preds[:, i] = clf.predict_proba(X)\n",
        "            else:\n",
        "                preds[:, i] = clf.predict(X)\n",
        "\n",
        "        # Soft voting: average of predicted probabilities\n",
        "        avg_preds = np.mean(preds, axis=1)\n",
        "        return avg_preds >= 0.5\n",
        "\n",
        "# Training and Evaluation\n",
        "logistic_regression = LogisticRegressionScratch(learning_rate=0.01, iterations=1000)\n",
        "random_forest = RandomForestScratch(n_estimators=10)\n",
        "svm = SVMScratch(learning_rate=0.001, n_iters=1000)\n",
        "\n",
        "# Create the voting classifier\n",
        "voting_classifier = VotingClassifierScratch([logistic_regression, random_forest, svm])\n",
        "\n",
        "# Train the ensemble model\n",
        "voting_classifier.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions with the voting classifier\n",
        "y_pred = voting_classifier.predict(x_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "# Output results\n",
        "print(f\"Voting Classifier Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Classification Report:\\n{classification_rep}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stack Ensemble\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FlBaYA9DtZ4-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ac4yGepeBNT-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65df0e7f-250e-4148-ed6f-97f1f8343e80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 99.95%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     71079\n",
            "           1       0.97      0.72      0.83       123\n",
            "\n",
            "    accuracy                           1.00     71202\n",
            "   macro avg       0.98      0.86      0.91     71202\n",
            "weighted avg       1.00      1.00      1.00     71202\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/creditcard.csv')\n",
        "\n",
        "# Feature and target variable\n",
        "X = data.drop(columns=['Class'])\n",
        "y = data['Class']\n",
        "\n",
        "# Splitting dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "# Standardizing the 'Time' and 'Amount' columns\n",
        "scaler = StandardScaler()\n",
        "X_train[['Time', 'Amount']] = scaler.fit_transform(X_train[['Time', 'Amount']])\n",
        "X_test[['Time', 'Amount']] = scaler.transform(X_test[['Time', 'Amount']])\n",
        "\n",
        "# Define base models\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
        "xgb_clf = xgb.XGBClassifier(n_estimators=100, scale_pos_weight=len(y_train) / sum(y_train), random_state=42)\n",
        "lr_clf = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42)\n",
        "\n",
        "# Define the stacking classifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[('rf', rf_clf), ('xgb', xgb_clf), ('lr', lr_clf)],\n",
        "    final_estimator=LogisticRegression()\n",
        ")\n",
        "\n",
        "# Train the stacking ensemble\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lhTkIubkZALz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}